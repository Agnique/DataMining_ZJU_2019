{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Walk Through Linear Models\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. Please check the pdf file for more details.*\n",
    "\n",
    "In this exercise you will:\n",
    "    \n",
    "- implement a whole bunch of **linear classifiers**\n",
    "- compare their performance and properties\n",
    "\n",
    "Please note that **YOU CANNOT USE ANY MACHINE LEARNING PACKAGE SUCH AS SKLEARN** for any homework, unless you are asked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic imports\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from mkdata import mkdata\n",
    "from plotdata import plotdata\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this skeleton or write your own.\n",
    "\n",
    "NOTE: Be becareful that the bias term is in the first element of weight, that is `y = np.sign(np.matmul(w_g.T, np.vstack((np.ones((1, X.shape[1])), X)))).T`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part1: Preceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perceptron import perceptron\n",
    "\n",
    "nRep = 1000 # number of replicates\n",
    "nTrain = 10 # number of training data\n",
    "for i in range(nRep):\n",
    "    X, y, w_f = mkdata(nTrain)\n",
    "    w_g, iters = perceptron(X, y)\n",
    "    # Compute training, testing error\n",
    "    # Sum up number of iterations\n",
    "\n",
    "# print('E_train is {}, E_test is {}'.format(E_train, E_test))\n",
    "# print('Average number of iterations is {}.'.format(avgIter))\n",
    "plotdata(X, y, w_f, w_g, 'Pecertron')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part2: Preceptron: Non-linearly separable case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrain = 100 # number of training data\n",
    "X, y, w_f = mkdata(nTrain, 'noisy')\n",
    "w_g, iters = perceptron(X, y)\n",
    "# print('E_train is {}, E_test is {}'.format(E_train, E_test))\n",
    "# print('Average number of iterations is {}.'.format(avgIter))\n",
    "plotdata(X, y, w_f, w_g, 'Pecertron Non-linearly separable')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part3: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_regression import linear_regression\n",
    "\n",
    "nRep = 1000  # number of replicates\n",
    "nTrain = 100 # number of training data\n",
    "\n",
    "for i in range(nRep):\n",
    "    X, y, w_f = mkdata(nTrain)\n",
    "    w_g = linear_regression(X, y)\n",
    "    # Compute training, testing error\n",
    "\n",
    "# print('E_train is {}, E_test is {}'.format(E_train, E_test))\n",
    "\n",
    "plotdata(X, y, w_f, w_g, 'Linear Regression');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part4: Linear Regression: noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nRep = 1000; # number of replicates\n",
    "nTrain = 100; # number of training data\n",
    "\n",
    "for i in range(nRep):\n",
    "    X, y, w_f = mkdata(nTrain, 'noisy')\n",
    "    w_g = linear_regression(X, y);\n",
    "    # Compute training, testing error\n",
    "\n",
    "# print('E_train is {}, E_test is {}'.format(E_train, E_test))\n",
    "\n",
    "plotdata(X, y, w_f, w_g, 'Linear Regression: noisy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part5: Linear Regression: poly_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "poly_train = sio.loadmat('poly_train')\n",
    "X, y = poly_train['X'], poly_train['y']\n",
    "poly_test = sio.loadmat('poly_test')\n",
    "X_test, y_test = poly_test['X_test'], poly_test['y_test']\n",
    "w = linear_regression(X, y)\n",
    "# Compute training, testing error\n",
    "\n",
    "# print('E_train is {}, E_test is {}'.format(E_train, E_test))\n",
    "\n",
    "# poly_fit with transform\n",
    "X_t = X  # CHANGE THIS LINE TO DO TRANSFORMATION\n",
    "X_test_t = X_test  # CHANGE THIS LINE TO DO TRANSFORMATION\n",
    "w = linear_regression(X_t, y)\n",
    "# Compute training, testing error\n",
    "# print('E_train is {}, E_test is {}'.format(E_train, E_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part6: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logistic import logistic\n",
    "\n",
    "# Since logistic regression outputs 0/1, we should adjust the label y.\n",
    "nRep = 100; # number of replicates\n",
    "nTrain = 100; # number of training data\n",
    "\n",
    "for i in range(nRep):\n",
    "    X, y, w_f = mkdata(nTrain)\n",
    "    w_g = logistic(X, y);\n",
    "    # Compute training, testing error\n",
    "\n",
    "# print('E_train is {}, E_test is {}'.format(E_train, E_test))\n",
    "\n",
    "plotdata(X, y, w_f, w_g, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part7: Logistic Regression: noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Since logistic regression outputs 0/1, we should adjust the label y.\n",
    "nRep = 100; # number of replicates\n",
    "nTrain = 100; # number of training data\n",
    "nTest = 10000; # number of training data\n",
    "\n",
    "for i in range(nRep):\n",
    "    X, y, w_f = mkdata(nTrain, 'noisy')\n",
    "    w_g = logistic(X, y)\n",
    "    # Compute training, testing error\n",
    "\n",
    "# print('E_train is {}, E_test is {}'.format(E_train, E_test))\n",
    "\n",
    "plotdata(X, y, w_f, w_g, 'Logistic Regression: noisy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part8: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svm import svm\n",
    "\n",
    "nRep = 1000; # number of replicates\n",
    "nTrain = 100; # number of training data\n",
    "\n",
    "for i in range(nRep):\n",
    "    X, y, w_f = mkdata(nTrain)\n",
    "    w_g, num_sc = svm(X, y)\n",
    "    # Compute training, testing error\n",
    "    # Sum up number of support vectors\n",
    "\n",
    "# print('E_train is {}, E_test is {}'.format(E_train, E_test))\n",
    "# print('Average number of support vectors is {}.'.format(avgNum))\n",
    "plotdata(X, y, w_f, w_g, 'SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also try SVM with **noisy** training data. Why it behaves poorly? \n",
    "You can also try to implement SVM with **slack variables**.  See how it behaves with noisy training data. However this is left as a bonus part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
